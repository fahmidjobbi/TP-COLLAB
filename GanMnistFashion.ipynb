{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahmidjobbi/TP-COLLAB/blob/main/GanMnistFashion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6YJWhLEjcEW"
      },
      "source": [
        "from numpy import zeros, ones, expand_dims, asarray\n",
        "from numpy.random import randn, randint\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Concatenate\n",
        "from keras.layers import LeakyReLU, Dropout, Embedding\n",
        "from keras.layers import BatchNormalization, Activation\n",
        "from keras import initializers\n",
        "from keras.initializers import RandomNormal\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "from math import sqrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgXsc387jtOx"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train1 = X_train.astype(np.float32) / 255\n",
        "#X_train2 = np.expand_dims(X_train1, axis=3)#ajouter une dimansion a la fin\n",
        "print(X_train1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEiEpEnEkcG9"
      },
      "source": [
        "print(X_train[20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jXL4RLJkE0E"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YvBi2OAjybm"
      },
      "source": [
        "plt.imshow(X_train[10],'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl08aWYvleHg"
      },
      "source": [
        "plt.figure(figsize =(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X_train1[i], cmap = plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zBVInW-mNSw"
      },
      "source": [
        "#generate a matrix randomly\n",
        "from numpy.random import randn, randint\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    x_input = randn(latent_dim * n_samples) #standard normal distribution vecteur of size latent_dim * n_samples\n",
        "    z_input = x_input.reshape(n_samples, latent_dim) #convert vector to matrix\n",
        "    return z_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL8BWka6qrY-"
      },
      "source": [
        "Z_input=generate_latent_points(100,64)\n",
        "print(Z_input)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z_input.shape"
      ],
      "metadata": {
        "id": "pDCe4bydcS4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrAezAdHQ_cn"
      },
      "source": [
        "plt.imshow(Z_input,'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDCzG1uImU-t"
      },
      "source": [
        "def generate_real_samples(X_train, n_samples): #extract 64 images randomly from original real data\n",
        "    ix = randint(0, X_train.shape[0], n_samples) \n",
        "    X = X_train[ix]  \n",
        "    y = ones((n_samples, 1)) \n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0MSFXtNoN6m"
      },
      "source": [
        "Real_Data,y_real=generate_real_samples(X_train1,64)\n",
        "Real_Data.shape\n",
        "#print(y_real)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol8j_uneon8I"
      },
      "source": [
        "y_real.shape\n",
        "print(y_real)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urmydlH8o5Qd"
      },
      "source": [
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "    z_input = generate_latent_points(latent_dim, n_samples)\n",
        "    images = generator.predict(z_input)  \n",
        "    y = zeros((n_samples, 1))\n",
        "    return images, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAoWW2RsrX0F"
      },
      "source": [
        "def define_generator(latent_dim): \n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    in_lat = Input(shape=(latent_dim,)) \n",
        "    gen = Dense(256, kernel_initializer=init)(in_lat)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = Dense(512, kernel_initializer=init)(gen)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = Dense(1024, kernel_initializer=init)(gen)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = Dense(28 * 28 * 1, kernel_initializer=init)(gen)\n",
        "    out_layer = Activation('sigmoid')(gen)\n",
        "    out_layer = Reshape((28, 28, 1))(out_layer)\n",
        "    model = Model(in_lat, out_layer)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWxgnPp3reNM"
      },
      "source": [
        "generator = define_generator(100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcuxWADRrLMM"
      },
      "source": [
        "images, y=generate_fake_samples(generator,100,64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTPYrKUqrXPs"
      },
      "source": [
        "images.shape\n",
        "images1=images.reshape(64,28,28)\n",
        "images1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS309FOIr4hc"
      },
      "source": [
        "plt.imshow(images1[63],'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuasZ6hcsPHf"
      },
      "source": [
        "def define_discriminator(in_shape=(28, 28, 1)):\n",
        "    in_image = Input(shape=in_shape)\n",
        "    fe = Flatten()(in_image)\n",
        "    fe = Dense(1024)(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    fe = Dropout(0.3)(fe)\n",
        "    fe = Dense(512)(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    fe = Dropout(0.3)(fe)\n",
        "    fe = Dense(256)(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    fe = Dropout(0.3)(fe)\n",
        "    out = Dense(1, activation='sigmoid')(fe)\n",
        "    model = Model(in_image, out)\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5) \n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BDvV3Nnt6JY"
      },
      "source": [
        "discriminator = define_discriminator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfvch06juJPt"
      },
      "source": [
        "def define_gan(g_model, d_model):\n",
        "    d_model.trainable = False #disabled the discriminator \n",
        "    gan_output = d_model(g_model.output)#in this we just use the generator \n",
        "    model = Model(g_model.input, gan_output)\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shuxBhW-OOzo"
      },
      "source": [
        "gan_model=define_gan(generator,discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75CRWCcQ568x"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raBupbjpurM-"
      },
      "source": [
        "gan_model = define_gan(generator, discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCy9J1hoMZc6"
      },
      "source": [
        " for i in range(200):\n",
        "   X_real, y_real = generate_real_samples(X_train1, 60000)\n",
        "   d_loss_r, d_acc_r = discriminator.train_on_batch(X_real, y_real)\n",
        "   X_fake, y_fake = generate_fake_samples(generator, 100, 60000)\n",
        "   d_loss_f, d_acc_f = discriminator.train_on_batch(X_fake, y_fake)\n",
        "   z_input = generate_latent_points(100, 60000) \n",
        "   y_gan = ones((60000, 1)) \n",
        "   g_loss, g_acc = gan_model.train_on_batch(z_input, y_gan)\n",
        "   print('>%d, dr[%.3f,%.3f], df[%.3f,%.3f], g[%.3f,%.3f]' \n",
        "         % (i+1, d_loss_r,d_acc_r, d_loss_f,d_acc_f, g_loss,g_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "images2, y=generate_fake_samples(generator,100,20)"
      ],
      "metadata": {
        "id": "Q8d2ruJtGYQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images2.shape\n",
        "images2=images2.reshape(20,28,28)\n",
        "images2.shape\n"
      ],
      "metadata": {
        "id": "1P0ivOiYGrPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(images2[4],'gray')"
      ],
      "metadata": {
        "id": "vlOSaAk2G7Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvvpUlWXv6Rq"
      },
      "source": [
        "def train(g_model, d_model, gan_model, X_train, latent_dim, n_epochs=2, n_batch=128):\n",
        "    bat_per_epo = int(X_train.shape[0] / n_batch)\n",
        "    n_steps = bat_per_epo * n_epochs\n",
        "    for i in range(n_steps):\n",
        "        X_real, y_real = generate_real_samples(X_train, n_batch)\n",
        "        d_loss_r, d_acc_r = d_model.train_on_batch(X_real, y_real)\n",
        "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_batch)\n",
        "        d_loss_f, d_acc_f = d_model.train_on_batch(X_fake, y_fake)\n",
        "        z_input = generate_latent_points(latent_dim, n_batch) \n",
        "        y_gan = ones((n_batch, 1)) \n",
        "        g_loss, g_acc = gan_model.train_on_batch(z_input, y_gan)\n",
        "        print('>%d, dr[%.3f,%.3f], df[%.3f,%.3f], g[%.3f,%.3f]' % (i+1, d_loss_r,d_acc_r, d_loss_f,d_acc_f, g_loss,g_acc))\n",
        "        if (i+1) % (bat_per_epo * 1) == 0:\n",
        "            summarize_performance(i, g_model, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7XULdNow5kS"
      },
      "source": [
        "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
        "    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "    X = (X + 1) / 2.0\n",
        "    for i in range(100):\n",
        "        plt.subplot(10, 10, 1 + i)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(X[i, :, :, 0], cmap='gray_r')\n",
        "    filename2 = 'model_%04d.h5' % (step+1)\n",
        "    g_model.save('/content/drive/My Drive/Colab Notebooks/'+ filename2)\n",
        "    print('>Saved: %s' % (filename2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBderVezxG7X"
      },
      "source": [
        "def save_plot(examples, n_examples):\n",
        "    for i in range(n_examples):\n",
        "        plt.subplot(sqrt(n_examples), sqrt(n_examples), 1 + i)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfVGpEn4vrrY"
      },
      "source": [
        "latent_dim = 100\n",
        "train(generator, discriminator, gan_model, X_train1, 100, n_epochs=2, n_batch=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDLsTHLY6WiC"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMxHW5HK6nvv"
      },
      "source": [
        "import os \n",
        "os.getcwd()\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faFX3mT5yEKM"
      },
      "source": [
        "model = load_model('model_0936.h5')\n",
        "latent_dim = 100\n",
        "n_examples = 100\n",
        "latent_points = generate_latent_points(latent_dim, n_examples)\n",
        "X  = model.predict(latent_points)\n",
        "X = (X + 1) / 2.0\n",
        "save_plot(X, n_examples)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}